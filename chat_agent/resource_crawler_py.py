# -*- coding: utf-8 -*-
"""resource_crawler.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hAroIHQUtgFJZcFvXSxmp0I5RMGU18rV
"""

import requests
from bs4 import BeautifulSoup
import csv

urls = [
    "https://www.fns.usda.gov/snap/students",
    "https://www.ncdhhs.gov/divisions/child-and-family-well-being/food-and-nutrition-services-food-stamps#FindOutifImEligible-4800",
    "https://www.canva.com/design/DAGO--9Hsns/IOvPx-qIvp2Wy-mwHzRhpg/view?utm_content=DAGO--9Hsns&utm_campaign=designshare&utm_medium=link&utm_source=editor#5",
    "https://packessentials.dasa.ncsu.edu/feed-the-pack/",
    "https://packessentials.dasa.ncsu.edu/support-feed-the-pack/",
    "https://packessentials.dasa.ncsu.edu/food-resources/"
]

# Output CSV file
csv_filename = "web_scraped_data.csv"

def fetch_data(url):
    """Fetches and parses webpage content"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        title = soup.title.string.strip() if soup.title else "No Title"
        paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
        content = " ".join(paragraphs)

        return {"url": url, "title": title, "content": content[:500]}  # Limit content size
    except Exception as e:
        print(f"Failed to fetch {url}: {e}")
        return {"url": url, "title": "Error", "content": str(e)}

def save_to_csv(data, filename):
    """Saves extracted data to a CSV file"""
    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=["url", "title", "content"])
        writer.writeheader()
        writer.writerows(data)

if __name__ == "__main__":
    scraped_data = [fetch_data(url) for url in urls]
    save_to_csv(scraped_data, csv_filename)
    print(f"Data saved to {csv_filename}")